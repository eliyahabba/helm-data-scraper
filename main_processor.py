import asyncio
import json
import logging
import os
import shutil
import signal
import atexit
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from typing import List

import colorama
import pandas as pd
# Create a colorful progress bar
from tqdm import tqdm
from tqdm.contrib.logging import logging_redirect_tqdm

from src.core.web_scraper import main as create_csv_main
from src.core.converter import main as convert_main
# Import functions from helm_downloader.py
from src.core.downloader import (
    download_tasks, log_info, log_success, log_error, log_warning, log_step
)
# Import centralized settings for paths and constants
from config.settings import (
    DEFAULT_DATA_DIR,
    TQDM_BAR_FORMAT,
    PROCESS_POOL_MAX_WORKERS,
    get_data_directories,
)

# Initialize colorama
colorama.init()

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("helm_processor.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("HELM_Processor")

# Create necessary directories (will be created dynamically based on args)


def read_tasks_from_csv(csv_file: str, adapter_method: str = None) -> List[str]:
    """
    Read tasks from CSV file in the format:
    ,Run,Model,Groups,Adapter method,Subject / Task

    Parameters:
        csv_file: Path to the CSV file
        adapter_method: Optional filter value for the 'Adapter method' column
                       (e.g., 'multiple_choice_joint')

    Returns a list of Run values (HELM catalog lines)
    """
    try:
        df = pd.read_csv(csv_file)
        total_rows = len(df)
        log_info(f"CSV file contains {total_rows} total rows", "üìä")

        if 'Run' not in df.columns:
            raise ValueError(f"CSV file {csv_file} doesn't have a 'Run' column")

        # Apply filter if adapter_method is provided
        if adapter_method and 'Adapter method' in df.columns:
            log_info(f"Filtering tasks by Adapter method: {adapter_method}", "üîç")
            filtered_df = df[df['Adapter method'] == adapter_method]
            filtered_rows = len(filtered_df)

            # Log the filtering results
            log_info(
                f"Found {filtered_rows} tasks with Adapter method '{adapter_method}' out of {total_rows} total tasks",
                "üìã")
            log_info(f"Using {filtered_rows}/{total_rows} rows ({filtered_rows / total_rows * 100:.1f}%)", "üìä")

            if filtered_rows == 0:
                log_warning(f"No tasks found with Adapter method '{adapter_method}'", "‚ö†Ô∏è")
                # Show available adapter methods for debugging
                available_methods = df['Adapter method'].unique()
                log_info(f"Available Adapter methods: {', '.join(str(m) for m in available_methods)}", "‚ÑπÔ∏è")

            df = filtered_df

        tasks_list = list(df['Run'])
        valid_tasks = [task for task in tasks_list if task and isinstance(task, str)]

        log_info(f"Read {len(valid_tasks)} valid tasks from {csv_file}", "üìÑ")
        if len(valid_tasks) < len(tasks_list):
            log_warning(f"Filtered out {len(tasks_list) - len(valid_tasks)} invalid or empty tasks", "‚ö†Ô∏è")

        return valid_tasks
    except Exception as e:
        log_error(f"Error reading CSV file {csv_file}: {str(e)}", "‚ùå")
        return []


def convert_data(data_dir: str, output_dir: str) -> str:
    """
    Convert the extracted data using convert_cluade.py
    Returns the path to the converted CSV file
    """
    # Create a unique output filename based on the data directory
    dir_basename = os.path.basename(data_dir)
    output_file = os.path.join(output_dir, f"{dir_basename}_converted.csv")

    log_step(f"Converting data from {data_dir} to {output_file}", "üîÑ")

    try:

        # Call the main function directly
        log_info(f"Running conversion", "‚öôÔ∏è")
        convert_main(data_dir, output_file)

        log_success(f"Conversion completed successfully")
        return output_file

    except ImportError as e:
        log_error(f"Error importing helm_converter: {e}")
        return None


def cleanup(zip_path: str) -> None:
    """
    Clean up temporary files and directories
    """
    log_step(f"Cleaning up temporary files", "üßπ")

    # Remove zip file
    if os.path.exists(zip_path):
        shutil.rmtree(zip_path) if os.path.isdir(zip_path) else os.remove(zip_path)
        log_info(f"Removed ZIP file: {zip_path}", "üóëÔ∏è")


def process_line(line: str, output_dir: str, benchmark: str, downloads_dir: str = None, 
                 keep_temp_files: bool = False, overwrite: bool = False) -> dict:
    """
    Process a single line from start to finish
    Returns a dictionary with the results
    """
    log_step(f"Processing line: {line}", "üîÑ")

    result = {
        "input_line": line,
        "status": "skipped",  # Default to skipped
        "downloaded_file": None,
        "converted_file": None,
        "entry_count": 0,
        "error": None
    }

    # Create a predictable output filename based on the line
    expected_output_file = os.path.join(output_dir, f"{line}_converted.csv")

    # Pre-compute the directory where downloads for this line will be stored
    saved_dir = os.path.join(downloads_dir, line) if downloads_dir else None

    def attempt_cleanup(reason: str) -> None:
        """Best-effort cleanup that never raises upstream."""
        if keep_temp_files:
            log_info(f"Skipping cleanup ({reason}); keep_temp_files=True", "üö´")
            return
        if not saved_dir:
            return
        try:
            cleanup(saved_dir)
        except Exception as cleanup_err:
            log_warning(f"Cleanup failed for '{saved_dir}' after {reason}: {cleanup_err}", "‚ö†Ô∏è")

    # Install signal handlers to attempt cleanup on termination
    prev_sigint = signal.getsignal(signal.SIGINT)
    prev_sigterm = signal.getsignal(signal.SIGTERM)

    def _handle_termination_signal(signum, frame):
        log_warning(f"Received termination signal ({signum}); attempting cleanup", "‚ö†Ô∏è")
        attempt_cleanup(f"signal {signum}")
        try:
            signal.signal(signum, signal.SIG_DFL)
        except Exception:
            pass
        os.kill(os.getpid(), signum)

    try:
        signal.signal(signal.SIGINT, _handle_termination_signal)
    except Exception:
        pass
    try:
        signal.signal(signal.SIGTERM, _handle_termination_signal)
    except Exception:
        pass

    # Also register process-exit cleanup as a last resort
    atexit.register(lambda: attempt_cleanup("process exit"))

    # Check if the output file already exists
    if os.path.exists(expected_output_file) and not overwrite:
        log_info(f"Output file already exists: {expected_output_file}", "üìã")

        # Count rows in the existing CSV file
        try:
            df_existing = pd.read_csv(expected_output_file)
            entry_count = len(df_existing)
            result["entry_count"] = entry_count
            log_info(f"File contains {entry_count} rows", "üî¢")
        except Exception as e:
            log_warning(f"Could not count rows in existing CSV: {e}", "‚ö†Ô∏è")

        log_success(f"Skipping processing for line: {line} (output file already exists)", "‚è≠Ô∏è")
        result["converted_file"] = expected_output_file
        return result

    # If we get here, we need to process the line
    result["status"] = "failed"  # Reset status to failed for processing

    try:
        # Use provided downloads_dir (should always be provided now)
        if downloads_dir is None:
            raise ValueError("downloads_dir parameter is required")
        
        # Download the data
        log_step(f"Starting download phase", "üîΩ")
        downloaded_files = download_tasks([line], output_dir=downloads_dir, benchmark=benchmark, overwrite=overwrite)
        if not downloaded_files:
            log_error(f"No files were downloaded")
            raise ValueError("No files were downloaded")

        # Convert the data
        log_step(f"Starting conversion phase", "üîÑ")
        converted_file = convert_data(saved_dir, output_dir)
        if not converted_file:
            log_error(f"Conversion failed")
            raise ValueError("Conversion failed")

        result["converted_file"] = converted_file

        # Count rows in the converted CSV file
        try:
            df_converted = pd.read_csv(converted_file)
            entry_count = len(df_converted)
            result["entry_count"] = entry_count
            log_info(f"Converted file contains {entry_count} rows", "üî¢")
        except Exception as e:
            log_warning(f"Could not count rows in converted CSV: {e}", "‚ö†Ô∏è")

        log_success(f"Conversion phase completed: {converted_file}", "üìÑ")

        # Clean up if not keeping temp files
        if not keep_temp_files:
            log_step(f"Starting cleanup phase", "üßπ")
            cleanup(saved_dir)
            log_success(f"Cleanup phase completed", "üßπ")
        else:
            log_info(f"Skipping cleanup (keep_temp_files=True)", "üö´")

        result["status"] = "success"
        log_success(f"Processing completed successfully for line: {line}", "üéâ")

    except Exception as e:
        log_error(f"Error processing line '{line}': {str(e)}")
        result["error"] = str(e)
    finally:
        # Best-effort cleanup on failure or interruption
        if result.get("status") != "success":
            attempt_cleanup("failure or interruption")
        # Restore previous signal handlers
        try:
            signal.signal(signal.SIGINT, prev_sigint)
        except Exception:
            pass
        try:
            signal.signal(signal.SIGTERM, prev_sigterm)
        except Exception:
            pass

    return result


def main(csv_file: str, output_dir: Path, benchmark: str, adapter_method: str = None, 
         downloads_dir: str = None, keep_temp_files: bool = False, overwrite: bool = False, 
         max_workers: int = PROCESS_POOL_MAX_WORKERS):
    """
    Main function to process all lines
    """
    # Process input from CSV, file, or list
    lines = read_tasks_from_csv(csv_file, adapter_method)

    if not lines:
        log_warning("No tasks to process after filtering", "‚ö†Ô∏è")
        return []

    results = []
    total_entries = 0

    # Configure colors for the progress bar
    bar_format = TQDM_BAR_FORMAT

    # Use tqdm with logging redirect to prevent progress bar disruption by log messages
    with logging_redirect_tqdm():
        # Create the progress bar
        pbar = tqdm(
            total=len(lines),
            desc=f"Processing {len(lines)} tasks",
            bar_format=bar_format,
            colour='green',
            ncols=100
        )

        # Parallel execution with a fixed-size process pool
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_line = {
                executor.submit(process_line, line, output_dir, benchmark, downloads_dir, keep_temp_files, overwrite): line for line in
                lines}

            for future in as_completed(future_to_line):
                line = future_to_line[future]
                try:
                    result = future.result()
                except Exception as e:
                    log_error(f"Error processing line '{line}': {str(e)}")
                    result = {
                        "input_line": line,
                        "status": "failed",
                        "downloaded_file": None,
                        "converted_file": None,
                        "entry_count": 0,
                        "error": str(e)
                    }

                results.append(result)
                total_entries += result.get("entry_count", 0)

                # Log the result
                if result["status"] == "success":
                    status_emoji = "‚úÖ"
                    pbar.colour = 'green'
                elif result["status"] == "skipped":
                    status_emoji = "‚è≠Ô∏è"
                    pbar.colour = 'blue'
                else:
                    status_emoji = "‚ùå"
                    pbar.colour = 'red'

                # Update progress bar with status and entry count
                pbar.set_postfix(
                    status=result["status"],
                    entries=result.get("entry_count", 0),
                    total_entries=total_entries
                )

                log_info(f"{status_emoji} Completed processing: {line} ({result.get('entry_count', 0)} entries)")

                # Update the progress bar
                pbar.update(1)

        # Close the progress bar when done
        pbar.close()

    # Generate a summary report
    success_count = sum(1 for r in results if r["status"] == "success")
    skipped_count = sum(1 for r in results if r["status"] == "skipped")
    failed_count = sum(1 for r in results if r["status"] == "failed")

    log_info(f"Processed {len(results)} lines:")
    log_info(f"  ‚úÖ Successful: {success_count}")
    log_info(f"  ‚è≠Ô∏è Skipped: {skipped_count}")
    log_info(f"  ‚ùå Failed: {failed_count}")
    log_info(f"  üî¢ Total entries across all files: {total_entries}")

    # Save results to a JSON file
    results_file = os.path.join(output_dir, "processing_results.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            "summary": {
                "total_lines": len(results),
                "successful": success_count,
                "skipped": skipped_count,
                "failed": failed_count,
                "total_entries": total_entries
            },
            "results": results
        }, f, indent=2, ensure_ascii=False)

    log_info(f"Results saved to {results_file}")
    log_success(f"Total entries processed: {total_entries}", "üî¢")

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Process HELM data from download to conversion")
    parser.add_argument("--benchmark", help="Benchmark name to process (e.g., 'lite', 'mmlu').", default="classic")
    parser.add_argument("--adapter-method", help="Filter tasks by Adapter method (e.g., 'multiple_choice_joint')")
    parser.add_argument("--keep-temp", action="store_true", help="Keep temporary files (zip and extracted directories)")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing output files")
    parser.add_argument("--max-workers", type=int, default=PROCESS_POOL_MAX_WORKERS,
                        help=f"Maximum number of parallel processes (default: {PROCESS_POOL_MAX_WORKERS})")
    parser.add_argument("--data-dir", type=str, default=str(DEFAULT_DATA_DIR),
                        help=f"Base data directory (default: {DEFAULT_DATA_DIR})")

    args = parser.parse_args()

    log_info(f"Starting HELM Data Processor", "üöÄ")
    log_info(f"Arguments: {args}", "üîß")

    # Create dynamic directory structure based on custom data directory
    custom_data_dir = Path(args.data_dir)
    data_dirs = get_data_directories(custom_data_dir)
    
    # Create necessary directories
    for dir_path in data_dirs.values():
        os.makedirs(dir_path, exist_ok=True)
    
    log_info(f"Using data directory: {data_dirs['data_dir']}", "üìÅ")

    # Define and create benchmark-specific directories
    benchmark_downloads_dir = data_dirs['downloads_dir'] / args.benchmark
    output_dir_path = data_dirs['processed_dir'] / args.benchmark
    os.makedirs(benchmark_downloads_dir, exist_ok=True)
    os.makedirs(output_dir_path, exist_ok=True)

    # Log paths
    log_info(f"Downloads for '{args.benchmark}' benchmark will be saved to: {benchmark_downloads_dir}", "üì•")
    log_info(f"Processed output for '{args.benchmark}' benchmark will be saved to: {output_dir_path}", "üìÇ")

    log_step(f"Processing benchmark: {args.benchmark}", "üìä")

    # Construct path to the CSV file using custom directory
    csv_to_process = data_dirs['benchmark_csvs_dir'] / f"helm_{args.benchmark}.csv"

    if not csv_to_process.exists() or args.overwrite:
        if not csv_to_process.exists():
            log_warning(f"CSV for benchmark '{args.benchmark}' not found. Downloading...", "üì•")
        else:
            log_info(f"Overwrite flag is set. Re-downloading CSV for benchmark '{args.benchmark}'.", "üì•")
        try:
            asyncio.run(create_csv_main(benchmark=args.benchmark, output_dir=str(data_dirs['benchmark_csvs_dir'])))
            log_success(f"Successfully created CSV: {csv_to_process}", "‚úÖ")
        except Exception as e:
            log_error(f"Failed to create CSV for benchmark '{args.benchmark}': {e}", "‚ùå")
            sys.exit(1)
    else:
        log_info(f"Found existing CSV for benchmark '{args.benchmark}': {csv_to_process}", "üìÑ")

    csv_to_process_str = str(csv_to_process)

    log_step(f"Processing from CSV file: {csv_to_process_str}", "üìã")
    main(csv_file=csv_to_process_str, output_dir=output_dir_path, benchmark=args.benchmark,
         adapter_method=args.adapter_method, downloads_dir=str(benchmark_downloads_dir),
         keep_temp_files=args.keep_temp, overwrite=args.overwrite, max_workers=args.max_workers)

    log_success(f"HELM Data Processor completed", "üèÅ")
